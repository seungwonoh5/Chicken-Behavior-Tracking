{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"detectron2.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"vnV-_MR4uXAv"},"source":["# install dependencies: \n","!pip install pyyaml==5.1 'pycocotools>=2.0.1'\n","# !pip install imgaug==0.2.5\n","!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.6/index.html\n","!gcc --version\n","\n","import torch, torchvision, random, os, cv2, glob, json, pylab, time\n","import numpy as np\n","import pandas as pd\n","from google.colab.patches import cv2_imshow\n","print(torch.__version__, torch.cuda.is_available())\n","assert torch.__version__.startswith(\"1.7\")\n","\n","# opencv is pre-installed on colab\n","import matplotlib.pyplot as plt\n","from pycocotools.coco import COCO\n","from pycocotools.cocoeval import COCOeval\n","import skimage.io as io\n","\n","# %reload_ext autoreload\n","# %autoreload\n","\n","torch.manual_seed(123)\n","torch.cuda.manual_seed(123)\n","np.random.seed(123)\n","random.seed(123)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MEEe5Kk-In_6"},"source":["from detectron2.utils.logger import setup_logger\n","setup_logger()\n","\n","from detectron2 import model_zoo\n","from detectron2.config import get_cfg\n","from detectron2.engine import DefaultTrainer, DefaultPredictor\n","from detectron2.utils.visualizer import Visualizer\n","from detectron2.data import MetadataCatalog, DatasetCatalog\n","from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n","from detectron2.data import build_detection_test_loader\n","from detectron2.structures import BoxMode\n","from detectron2.data.datasets import register_coco_instances\n","from detectron2.utils.visualizer import ColorMode"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TIXn3jYk-jmW"},"source":["def extract_labelme(json_file):\n","    \"\"\"extracts only the relevant information in the annotation files(.json) created from LabelMe  \n","\n","    :params \n","        - json_file: str, The path to the folder where your .json files are\n","    \"\"\"\n","\n","    f = open(json_file, \"r\") # open JSON file\n","    data = json.load(f) # return JSON file object as dict\n","\n","    # extract only meaningful information for all objects in an image\n","    filename = data[\"imagePath\"].split('/')[-1]\n","    width = int(data[\"imageWidth\"])\n","    height = int(data[\"imageHeight\"])\n","    obj_list = data['shapes'] # list of dicts\n","\n","    return filename, width, height, obj_list\n","\n","\n","def check_annotations(dataset_name, d):\n","     \"\"\"load annotation files(.json) created from LabelMe and create a list[dict] to be registered for Detectron2\n","\n","    :params\n","        - dataset_name: str, The path to the folder where your .json files are\n","        - d: \n","    \"\"\"\n","    metadata = MetadataCatalog.get(dataset_name)\n","\n","    print(d[\"image_id\"])\n","    img = cv2.imread(d[\"file_name\"])\n","    visualizer = Visualizer(img[:, :, ::-1], metadata=metadata, scale=0.5)\n","    out = visualizer.draw_dataset_dict(d)\n","    cv2_imshow(out.get_image()[:, :, ::-1])\n","\n","\n","def labelme_to_detectron(json_dir, label_type):\n","    \"\"\"load annotation files(.json) created from LabelMe and create a list[dict] to be registered for Detectron2\n","\n","    :params\n","        - json_dir: str, The path to the folder where your .json files are\n","    \"\"\"\n","\n","    dataset_dicts = [] # list[dict]\n","    dirFiles = sorted(glob.glob(os.path.join(json_dir, '*.json')))\n","\n","    if label_type == 'pos' or label_type == 'behavior':\n","        df = pd.read_csv('{}/{}.csv'.format(json_dir, json_dir.split('/')[-1]))\n","    \n","    for json_file in dirFiles:\n","\n","        filename, width, height, obj_list = extract_labelme(json_file) # extract annotation info from labelMe json file   \n","\n","        obj_list = {obj[\"label\"]:np.array(obj[\"points\"]) for obj in obj_list} # rearrange list[dict] into dict{int: shape (8,2)}\n","\n","        record = {} # dict for single image\n","\n","        record[\"file_name\"] = json_dir + '/' + filename # must be full path to where the image is\n","        record[\"height\"] = height\n","        record[\"width\"] = width\n","        record[\"image_id\"] = int(filename.split('.')[0]) # 55555555.jpg (must be unique across #s)\n","\n","        if record[\"image_id\"] % 100 == 0:\n","            print(filename)\n","\n","        objs = [] # list[dict]\n","\n","        for key, value in obj_list.items():\n","            px = value[:,0]\n","            py = value[:,1]\n","            polygon = []\n","\n","            for i in range(value.shape[0]):\n","                polygon.append(value[i][0]) \n","                polygon.append(value[i][1])\n","            \n","            obj = {\n","                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n","                \"bbox_mode\": BoxMode.XYXY_ABS,\n","                \"category_id\": int(key)-1,\n","                \"segmentation\": [polygon]\n","            }\n","            \n","            # find a row in the csv file that has the current image and the bird id\n","            if label_type == 'id':\n","                obj['category_id'] = 0\n","                objs.append(obj)\n","            \n","            # if label_type == 'pos' or 'behavior':\n","            else:\n","                # get the csv label matching chicken x in image y in the json file\n","                image_num = (df['Image'] == record['image_id'])\n","                chick_id = (df['Bird #'] == (obj['category_id']+1))\n","                row = df[image_num & chick_id]\n","                \n","                if row.empty:\n","                    print('DataFrame is empty!')\n","                \n","                else:\n","                    # select posture columns to get the one-hot-vector  \n","                    if label_type == 'pos':\n","                        y_behavior = row.loc[:,['STD', 'SIT']].to_numpy()\n","\n","                    if label_type == 'behavior':\n","                        y_behavior = row.loc[:,['EAT', 'DRK', 'PRE', 'FOR']].to_numpy()\n","\n","                    y_behavior = y_behavior[0]\n","\n","                    # get only STD and SIT from the row    \n","                    if np.sum(y_behavior) != 0:\n","                        obj['category_id'] = int(np.argmax(y_behavior))\n","                        objs.append(obj)\n","               \n","        record[\"annotations\"] = objs\n","    \n","        dataset_dicts.append(record) # list[dict]\n","        \n","    print(\"CONVERTED TO DETECTRON2 FORMAT!\")\n","    return dataset_dicts\n","\n","\n","def register_datasets(dataset_name, data_dir, classes, label_type):\n","    \"\"\"register chicken videos for training and evalating\n","\n","    :params \n","        - dataset_name: str, dataset registered on Detectron2 DatasetCatalog\n","        - data_dir: str, path where the annotation files are \n","        - classes: list[str], list of classes\n","        - label_type: str, task that you want to train\n","    \"\"\"\n","\n","    # if dataset name is already in the catalog, then erase it to for re-registration\n","    if dataset_name in DatasetCatalog.list():\n","        DatasetCatalog.remove(dataset_name)\n","        MetadataCatalog.remove(dataset_name)\n","\n","    DatasetCatalog.register(dataset_name, lambda data_dir=data_dir, label_type=label_type: labelme_to_detectron(data_dir, label_type)) # register your dataset with a name and a function to convert\n","    MetadataCatalog.get(dataset_name).thing_classes = classes # register list of classes to your metadata for your dataset\n","\n","    print('{} registered successfully'.format(dataset_name))\n","\n","\n","def train_detector(dataset_name, model_file, output_path, max_iter, classes, lr):\n","    \"\"\"train the detector with your registered dataset and save the trained weights to an output path\n","\n","    returns: \n","        - trainer: object detector(nn.module)\n","        - cfg: config file for the modules\n","\n","    params:\n","        - dataset_name: registered dataset\n","        - model_file: model config file for setting up model for training\n","        - output_path: path to saving trained weights\n","        - max_iter: number of epochs to train\n","        - classes: list of classes used for training dataset\n","        - lr: constant learning rate\n","    \"\"\"\n","    cfg = get_cfg()\n","    cfg.merge_from_file(model_zoo.get_config_file(model_file)) # select model of your choice\n","    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model_file) # to fine-tune it on our dataset, initialize from model zoo (pretrained weights from ImageNet)\n","    cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(classes)  # number of classes \n","\n","    # pick all the datasets you want to train with\n","    cfg.DATASETS.TRAIN = ('{}286'.format(dataset_name),'{}287'.format(dataset_name),'{}288'.format(dataset_name),'{}289'.format(dataset_name),'{}290'.format(dataset_name),\n","                          '{}213'.format(dataset_name),'{}214'.format(dataset_name),'{}215'.format(dataset_name))\n","    cfg.OUTPUT_DIR = output_path\n","\n","    # set up hyperparameters\n","    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 32   # faster, and good enough for this toy dataset (default: 512)\n","    cfg.DATALOADER.NUM_WORKERS = 2\n","    cfg.SOLVER.IMS_PER_BATCH = 2\n","    cfg.SOLVER.BASE_LR = lr  # pick a good LR\n","    cfg.SOLVER.MAX_ITER = max_iter # 300 iterations seems good enough for this toy dataset; you may need to train longer for a practical dataset\n","\n","    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True) # make directory for your output path if doesn't exist\n","\n","    trainer = DefaultTrainer(cfg) \n","    trainer.resume_or_load(resume=False)\n","    trainer.train()\n","\n","    return trainer, cfg\n","\n","\n","def get_detector(config_file, nms_thresh, data_type, classes, weight_dir=None):\n","    \"\"\" for each image you make an prediction and find the indexes of correctly classified rois\n","\n","    returns: \n","        - model: object detector(nn.module)\n","\n","    params:\n","        - config_file:\n","        - nms_thresh: it determins the minimum confidence threshold for rois to survive for NMS\n","        - data_type: \n","        - classes:\n","        - weight_file: \n","    \"\"\"\n","    cfg = get_cfg()\n","\n","    # add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n","    cfg.merge_from_file(model_zoo.get_config_file(config_file))\n","\n","    # set threshold for this model\n","    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = nms_thresh \n","    cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(classes)  # number of classes \n","\n","    # Upload initial or trained weights\n","    if weight_dir is None:\n","        cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(config_file) # use the pretrained weights\n","\n","    else:\n","        cfg.MODEL.WEIGHTS = os.path.join(weight_dir, 'model_final.pth') # use the custom trained weights\n","\n","    # load DefaultPredictor or model\n","    if data_type == 'model':\n","        model = build_model(cfg)  # returns a torch.nn.Module\n","        DetectionCheckpointer(model).load(cfg.MODEL.WEIGHTS)\n","        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","        model = model.to(device)\n","        model.eval()\n","\n","    if data_type == 'predictor':\n","        model = DefaultPredictor(cfg)\n","\n","    return model\n","\n","\n","def predict_video(dataset_name, model, model_weight_path):\n","    \"\"\"make predictions on test frames by using your trained object detector and convert predicted frames into a video\n","\n","    params:\n","        - dataset_name: str, the dataset that you want to test your trained detector on (must be registered via func 'register_datasets')\n","        - model: \n","        - model_weight_path:\n","        - v_num:\n","    \"\"\" \n","\n","    frames = []\n","    dataset_dicts = DatasetCatalog.get(dataset_name) # load dataset that you want to test\n","    metadata = MetadataCatalog.get(dataset_name) # get metadata to use on Visualizer\n","    start_ts = time.time()\n","\n","    for idx, d in enumerate(dataset_dicts):\n","        im = cv2.imread(d[\"file_name\"])\n","\n","        # make a prediction on each image and draw predictions on the image\n","        outputs = model(im)\n","        v = Visualizer(im[:, :, ::-1], metadata=metadata, scale=0.5)\n","        out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n","        frames.append(out.get_image()[:, :, ::-1])\n","\n","        if idx % 100 == 0: \n","            cv2_imshow(out.get_image()[:, :, ::-1])\n","            check_annotations(dataset_name, d)\n","\n","    pathOut = os.path.join(model_weight_path, '{}_{}.avi'.format(dataset_name, model_weight_path.split('/')[-1]))\n","    fourcc = cv2.VideoWriter_fourcc(*'XVID') # a 4-byte code used to specify the video codec\n","    fps = 5\n","    size = (frames[0].shape[1], frames[0].shape[0])\n","\n","    out = cv2.VideoWriter(pathOut, fourcc, fps, size)\n","\n","    # writing video to a image array\n","    for i in range(len(frames)):\n","        out.write(frames[i]) \n","\n","    out.release()\n","    print(\"time elapsed: {}\".format((time.time() - start_ts)/60))\n","    print('Prediction Video demo saved in {}'.format(pathOut))\n","\n","def run_inference(img, model):\n","    start_ts = time.time()\n","    outputs = model(im)\n","    print(\"time elapsed: {}\".format((time.time() - start_ts)/60))\n","\n","    return outputs"],"execution_count":null,"outputs":[]}]}